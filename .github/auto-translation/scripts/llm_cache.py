#!/usr/bin/env python3
"""
JHipsteræ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ 
LLMã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ  - SQLiteãƒ™ãƒ¼ã‚¹
"""

import sqlite3
import hashlib
import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LLMCache:
    """SQLiteãƒ™ãƒ¼ã‚¹ã®LLMã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, cache_db_path: Optional[str] = None):
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
        
        Args:
            cache_db_path: SQLiteãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã€‚Noneã®å ´åˆã¯.out/llm_cache.dbã‚’ä½¿ç”¨
        """
        if cache_db_path is None:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¦‹ã¤ã‘ã‚‹
            current = Path(__file__).parent
            while current != current.parent:
                if (current / '.git').exists() or (current / 'package.json').exists():
                    project_root = current
                    break
                current = current.parent
            else:
                project_root = Path.cwd()
            
            # .outãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            cache_dir = project_root / ".github" / "auto-translation" / ".out"
            cache_dir.mkdir(parents=True, exist_ok=True)
            cache_db_path = str(cache_dir / "llm_cache.db")
        
        self.db_path = cache_db_path
        self.init_database()
        
        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "cache_stores": 0
        }
    
    def init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS translation_cache (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    file_path TEXT NOT NULL,
                    upstream_sha TEXT NOT NULL,
                    line_no INTEGER NOT NULL,
                    src_hash TEXT NOT NULL,
                    original_content TEXT NOT NULL,
                    translated_content TEXT NOT NULL,
                    model_name TEXT NOT NULL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    accessed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    access_count INTEGER DEFAULT 1,
                    UNIQUE(file_path, upstream_sha, line_no, src_hash)
                )
            """)
            
            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_cache_lookup 
                ON translation_cache(file_path, upstream_sha, line_no, src_hash)
            """)
            
            cursor.execute("""
                CREATE INDEX IF NOT EXISTS idx_cache_accessed_at 
                ON translation_cache(accessed_at)
            """)
            
            conn.commit()
            logger.info(f"âœ… Cache database initialized: {self.db_path}")
    
    def _generate_content_hash(self, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®SHA256ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()[:16]  # çŸ­ç¸®ç‰ˆ
    
    def get_cache_key(self, file_path: str, upstream_sha: str, line_no: int, content: str) -> Tuple[str, str, int, str]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        src_hash = self._generate_content_hash(content)
        return (file_path, upstream_sha, line_no, src_hash)
    
    def get(self, file_path: str, upstream_sha: str, line_no: int, content: str) -> Optional[str]:
        """
        ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ç¿»è¨³çµæœã‚’å–å¾—
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            upstream_sha: ã‚¢ãƒƒãƒ—ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥
            line_no: è¡Œç•ªå·
            content: å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            
        Returns:
            ç¿»è¨³çµæœã¾ãŸã¯Noneï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ã®å ´åˆï¼‰
        """
        file_path, upstream_sha, line_no, src_hash = self.get_cache_key(file_path, upstream_sha, line_no, content)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    SELECT translated_content, model_name, access_count
                    FROM translation_cache 
                    WHERE file_path = ? AND upstream_sha = ? AND line_no = ? AND src_hash = ?
                """, (file_path, upstream_sha, line_no, src_hash))
                
                result = cursor.fetchone()
                
                if result:
                    translated_content, model_name, access_count = result
                    
                    # ã‚¢ã‚¯ã‚»ã‚¹æƒ…å ±ã‚’æ›´æ–°
                    cursor.execute("""
                        UPDATE translation_cache 
                        SET accessed_at = CURRENT_TIMESTAMP, access_count = access_count + 1
                        WHERE file_path = ? AND upstream_sha = ? AND line_no = ? AND src_hash = ?
                    """, (file_path, upstream_sha, line_no, src_hash))
                    
                    conn.commit()
                    
                    self.stats["cache_hits"] += 1
                    logger.debug(f"ğŸ¯ Cache HIT: {file_path}:{line_no} (model: {model_name}, access: {access_count + 1})")
                    return translated_content
                else:
                    self.stats["cache_misses"] += 1
                    logger.debug(f"âŒ Cache MISS: {file_path}:{line_no}")
                    return None
                    
        except sqlite3.Error as e:
            logger.error(f"âŒ Cache lookup error: {e}")
            self.stats["cache_misses"] += 1
            return None
    
    def put(self, file_path: str, upstream_sha: str, line_no: int, original_content: str, 
            translated_content: str, model_name: str = "gemini-1.5-flash") -> bool:
        """
        ç¿»è¨³çµæœã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        
        Args:
            file_path: ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            upstream_sha: ã‚¢ãƒƒãƒ—ã‚¹ãƒˆãƒªãƒ¼ãƒ ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥
            line_no: è¡Œç•ªå·
            original_content: å…ƒã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
            translated_content: ç¿»è¨³çµæœ
            model_name: ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å
            
        Returns:
            ä¿å­˜æˆåŠŸã—ãŸå ´åˆTrue
        """
        file_path, upstream_sha, line_no, src_hash = self.get_cache_key(file_path, upstream_sha, line_no, original_content)
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # UPSERT (INSERT OR REPLACE)
                cursor.execute("""
                    INSERT OR REPLACE INTO translation_cache 
                    (file_path, upstream_sha, line_no, src_hash, original_content, translated_content, model_name)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (file_path, upstream_sha, line_no, src_hash, original_content, translated_content, model_name))
                
                conn.commit()
                
                self.stats["cache_stores"] += 1
                logger.debug(f"ğŸ’¾ Cache STORE: {file_path}:{line_no}")
                return True
                
        except sqlite3.Error as e:
            logger.error(f"âŒ Cache store error: {e}")
            return False
    
    def get_stats(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total_requests = self.stats["cache_hits"] + self.stats["cache_misses"]
        hit_rate = (self.stats["cache_hits"] / total_requests * 100) if total_requests > 0 else 0
        
        stats = self.stats.copy()
        stats.update({
            "total_requests": total_requests,
            "hit_rate_percent": round(hit_rate, 2)
        })
        
        return stats
    
    def get_database_stats(self) -> Dict:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                # ç·ã‚¨ãƒ³ãƒˆãƒªæ•°
                cursor.execute("SELECT COUNT(*) FROM translation_cache")
                total_entries = cursor.fetchone()[0]
                
                # ãƒ•ã‚¡ã‚¤ãƒ«åˆ¥ã‚¨ãƒ³ãƒˆãƒªæ•°
                cursor.execute("""
                    SELECT file_path, COUNT(*) as count 
                    FROM translation_cache 
                    GROUP BY file_path 
                    ORDER BY count DESC 
                    LIMIT 10
                """)
                top_files = cursor.fetchall()
                
                # æœ€æ–°ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒª
                cursor.execute("""
                    SELECT file_path, upstream_sha, created_at 
                    FROM translation_cache 
                    ORDER BY created_at DESC 
                    LIMIT 5
                """)
                recent_entries = cursor.fetchall()
                
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚ºï¼ˆæ¦‚ç®—ï¼‰
                cursor.execute("PRAGMA page_count")
                page_count = cursor.fetchone()[0]
                cursor.execute("PRAGMA page_size")
                page_size = cursor.fetchone()[0]
                db_size_bytes = page_count * page_size
                
                return {
                    "total_entries": total_entries,
                    "top_files": [{"file": f, "count": c} for f, c in top_files],
                    "recent_entries": [{"file": f, "sha": s, "created": c} for f, s, c in recent_entries],
                    "database_size_mb": round(db_size_bytes / (1024 * 1024), 2)
                }
                
        except sqlite3.Error as e:
            logger.error(f"âŒ Database stats error: {e}")
            return {"error": str(e)}
    
    def cleanup_old_entries(self, days_old: int = 30) -> int:
        """å¤ã„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ³ãƒˆãƒªã‚’å‰Šé™¤"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                
                cursor.execute("""
                    DELETE FROM translation_cache 
                    WHERE accessed_at < datetime('now', '-{} days')
                """.format(days_old))
                
                deleted_count = cursor.rowcount
                conn.commit()
                
                logger.info(f"ğŸ§¹ Cleaned up {deleted_count} old cache entries (older than {days_old} days)")
                return deleted_count
                
        except sqlite3.Error as e:
            logger.error(f"âŒ Cleanup error: {e}")
            return 0
    
    def clear_cache(self) -> bool:
        """å…¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤"""
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.cursor()
                cursor.execute("DELETE FROM translation_cache")
                conn.commit()
                
                logger.info("ğŸ§¹ Cache cleared completely")
                return True
                
        except sqlite3.Error as e:
            logger.error(f"âŒ Cache clear error: {e}")
            return False


def main():
    """CLI ãƒ†ã‚¹ãƒˆç”¨ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LLM Cache Management")
    subparsers = parser.add_subparsers(dest='command', help='Available commands')
    
    # stats ã‚³ãƒãƒ³ãƒ‰
    stats_parser = subparsers.add_parser('stats', help='Show cache statistics')
    
    # cleanup ã‚³ãƒãƒ³ãƒ‰
    cleanup_parser = subparsers.add_parser('cleanup', help='Cleanup old cache entries')
    cleanup_parser.add_argument('--days', type=int, default=30, help='Days to keep (default: 30)')
    
    # clear ã‚³ãƒãƒ³ãƒ‰
    clear_parser = subparsers.add_parser('clear', help='Clear all cache')
    
    args = parser.parse_args()
    
    cache = LLMCache()
    
    if args.command == 'stats':
        runtime_stats = cache.get_stats()
        db_stats = cache.get_database_stats()
        
        print("=== Runtime Statistics ===")
        print(json.dumps(runtime_stats, indent=2))
        print("\n=== Database Statistics ===")
        print(json.dumps(db_stats, indent=2))
        
    elif args.command == 'cleanup':
        deleted = cache.cleanup_old_entries(args.days)
        print(f"Deleted {deleted} old entries")
        
    elif args.command == 'clear':
        if cache.clear_cache():
            print("Cache cleared successfully")
        else:
            print("Failed to clear cache")
    
    else:
        parser.print_help()


if __name__ == "__main__":
    main()