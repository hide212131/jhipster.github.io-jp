#!/usr/bin/env python3
"""
JHipsteræ—¥æœ¬èªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè‡ªå‹•ç¿»è¨³ã‚·ã‚¹ãƒ†ãƒ  - LLMå‘¼ã³å‡ºã—å…±é€šå±¤
"""

import asyncio
import random
import time
from threading import Semaphore, Lock
from typing import Optional, Dict, Any
from collections import deque
from datetime import datetime, timedelta

import google.generativeai as genai

from .config import GeminiConfig


class RateLimiter:
    """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†"""
    
    def __init__(self, requests_per_minute: int):
        self.requests_per_minute = requests_per_minute
        self.requests = deque()
        self.lock = Lock()
    
    def can_proceed(self) -> bool:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯"""
        with self.lock:
            now = datetime.now()
            # 1åˆ†ä»¥ä¸Šå¤ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‰Šé™¤
            while self.requests and self.requests[0] < now - timedelta(minutes=1):
                self.requests.popleft()
            
            return len(self.requests) < self.requests_per_minute
    
    def record_request(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨˜éŒ²"""
        with self.lock:
            self.requests.append(datetime.now())
    
    def wait_time(self) -> float:
        """æ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§ã®å¾…æ©Ÿæ™‚é–“ã‚’è¨ˆç®—"""
        with self.lock:
            if len(self.requests) < self.requests_per_minute:
                return 0.0
            
            # æœ€å¤ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰1åˆ†å¾Œã¾ã§å¾…æ©Ÿ
            oldest = self.requests[0]
            wait_until = oldest + timedelta(minutes=1)
            wait_seconds = (wait_until - datetime.now()).total_seconds()
            return max(0.0, wait_seconds)


class GeminiClient:
    """Gemini APIå‘¼ã³å‡ºã—ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶å¾¡ãƒ»ãƒªãƒˆãƒ©ã‚¤ãƒ»ãƒ¢ãƒ‡ãƒ«åˆ‡æ›¿å¯¾å¿œï¼‰"""
    
    def __init__(self, config: GeminiConfig):
        self.config = config
        
        # APIè¨­å®š
        genai.configure(api_key=config.api_key)
        
        # ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._models: Dict[str, Any] = {}
        
        # ä¸¦åˆ—åˆ¶å¾¡ç”¨ã‚»ãƒãƒ•ã‚©
        self.semaphore = Semaphore(config.max_concurrent_requests)
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ç®¡ç†
        self.rate_limiter = RateLimiter(config.requests_per_minute)
        
        print(f"âœ… Gemini client initialized with max_concurrent={config.max_concurrent_requests}, "
              f"rate_limit={config.requests_per_minute}/min")
    
    def _get_model(self, model_name: str):
        """ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
        if model_name not in self._models:
            self._models[model_name] = genai.GenerativeModel(model_name)
        return self._models[model_name]
    
    def _calculate_delay(self, attempt: int) -> float:
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã«ã‚ˆã‚‹é…å»¶æ™‚é–“ã‚’è¨ˆç®—"""
        delay = self.config.base_delay * (2 ** attempt)
        delay = min(delay, self.config.max_delay)
        
        # ã‚¸ãƒƒã‚¿ãƒ¼ã‚’è¿½åŠ 
        jitter = random.uniform(0, self.config.jitter_max)
        return delay + jitter
    
    def _wait_for_rate_limit(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«å¾“ã£ã¦å¾…æ©Ÿ"""
        wait_time = self.rate_limiter.wait_time()
        if wait_time > 0:
            print(f"ğŸ• Rate limit: waiting {wait_time:.1f}s")
            time.sleep(wait_time)
    
    def generate_content(self, 
                        prompt: str, 
                        content: str = "", 
                        model_override: Optional[str] = None) -> Optional[str]:
        """
        ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
        
        Args:
            prompt: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            content: ç¿»è¨³å¯¾è±¡ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ï¼ˆãƒ¢ãƒ‡ãƒ«é¸æŠã«ä½¿ç”¨ï¼‰
            model_override: ãƒ¢ãƒ‡ãƒ«åã‚’å¼·åˆ¶æŒ‡å®š
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã€å¤±æ•—æ™‚ã¯None
        """
        # ãƒ¢ãƒ‡ãƒ«é¸æŠ
        if model_override:
            model_name = model_override
        else:
            model_name = self.config.select_model(content or prompt)
        
        # ã‚»ãƒãƒ•ã‚©ã«ã‚ˆã‚‹ä¸¦åˆ—åˆ¶å¾¡
        with self.semaphore:
            return self._generate_content_with_retry(prompt, model_name)
    
    def _generate_content_with_retry(self, prompt: str, model_name: str) -> Optional[str]:
        """ãƒªãƒˆãƒ©ã‚¤ä»˜ãã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        model = self._get_model(model_name)
        
        for attempt in range(self.config.max_retries + 1):
            try:
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ãƒã‚§ãƒƒã‚¯
                self._wait_for_rate_limit()
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆå®Ÿè¡Œ
                print(f"ğŸ¤– Calling {model_name} (attempt {attempt + 1})")
                
                response = model.generate_content(
                    prompt,
                    generation_config=genai.types.GenerationConfig(
                        temperature=0.1,  # ç¿»è¨³ã®ä¸€è²«æ€§ã®ãŸã‚ä½ã‚ã«è¨­å®š
                    )
                )
                
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¨˜éŒ²
                self.rate_limiter.record_request()
                
                if response.text:
                    return response.text.strip()
                else:
                    print(f"âš ï¸ Empty response from {model_name}")
                    
            except Exception as e:
                print(f"âŒ Error with {model_name} (attempt {attempt + 1}): {e}")
                
                # æœ€å¾Œã®è©¦è¡Œã§ãªã„å ´åˆã¯å¾…æ©Ÿ
                if attempt < self.config.max_retries:
                    delay = self._calculate_delay(attempt)
                    print(f"â³ Retrying in {delay:.1f}s...")
                    time.sleep(delay)
        
        print(f"âŒ All attempts failed for {model_name}")
        return None


class MockGeminiClient:
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""
    
    def __init__(self, config: GeminiConfig):
        self.config = config
        self.call_count = 0
        self.model_usage = {}
        print("ğŸ§ª Mock Gemini client initialized")
    
    def generate_content(self, 
                        prompt: str, 
                        content: str = "", 
                        model_override: Optional[str] = None) -> Optional[str]:
        """ãƒ¢ãƒƒã‚¯å®Ÿè£…"""
        self.call_count += 1
        
        # ãƒ¢ãƒ‡ãƒ«é¸æŠã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
        if model_override:
            model_name = model_override
        else:
            model_name = self.config.select_model(content or prompt)
        
        # ä½¿ç”¨çµ±è¨ˆã‚’è¨˜éŒ²
        self.model_usage[model_name] = self.model_usage.get(model_name, 0) + 1
        
        print(f"ğŸ§ª Mock call #{self.call_count} using {model_name}")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸºã¥ããƒ€ãƒŸãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹
        if "ç¿»è¨³" in prompt:
            # å…¥åŠ›ã®è¡Œæ•°ã‚’ä¿æŒã—ã¦ãƒ€ãƒŸãƒ¼ç¿»è¨³ã‚’è¿”ã™
            input_lines = content.split('\n') if content else prompt.split('\n')
            mock_translation = '\n'.join([f"ç¿»è¨³ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ_{i+1}" for i in range(len(input_lines))])
            return mock_translation
        else:
            return "Mock response from Gemini"
    
    def get_stats(self) -> Dict[str, Any]:
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        return {
            "total_calls": self.call_count,
            "model_usage": self.model_usage.copy()
        }


def create_gemini_client(config: GeminiConfig, mock: bool = False) -> GeminiClient:
    """Geminiã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½œæˆ"""
    if mock or config.api_key == "fake" or config.api_key.startswith("test_"):
        return MockGeminiClient(config)
    else:
        return GeminiClient(config)